# MCP Test Management - AI-Powered Test Execution

## Overview

The HelpMeTest MCP server now includes powerful test management capabilities that allow your AI assistant to discover, organize, and execute your test suites through natural conversation.

## New MCP Tools

### `helpmetest_status_test`
**Purpose:** Get status of all tests in the helpmetest system
**Arguments:** 
- `verbose` (optional, default: false): Enable verbose output with full test content, descriptions, and execution details
**Returns:** Complete test inventory with status, IDs, names, descriptions, and tags

### `helpmetest_run_test`  
**Purpose:** Execute tests by name, tag, or ID with real-time results
**Arguments:** 
- `identifier` (required): Test name, tag (tag:tagname), or ID to run
**Returns:** Complete execution results with timing, status, and detailed logs

### `helpmetest_status`
**Purpose:** Get comprehensive status of all tests and health checks in the helpmetest system
**Arguments:**
- `verbose` (optional, default: false): Enable verbose output with test content, descriptions, and additional healthcheck data
**Returns:** Complete system status including tests and health checks

### `helpmetest_status_health`
**Purpose:** Get status of all health checks in the helpmetest system
**Arguments:**
- `verbose` (optional, default: false): Enable verbose output with additional healthcheck metadata, heartbeat data, and debug information
**Returns:** Complete health check inventory with status, metadata, and heartbeat information

### `helpmetest_updates`
**Purpose:** Get recent company updates from the updates feed
**Arguments:**
- `limit` (optional, default: 20): Number of updates to retrieve
- `tags` (optional): Filter updates by specific tags (e.g., "test:deletion")
**Returns:** Recent company updates with tags, data, and timestamps

### `helpmetest_restore_test`
**Purpose:** Restore a previously deleted test from the updates feed
**Arguments:**
- `update_id` (required): ID of the update record containing the deleted test data
**Returns:** Restoration status and restored test details

For detailed examples and usage of the updates feed, see [Updates Feed Integration](./updates-feed.md).

## Natural Language Examples

### Test Discovery
```
You: "What tests do I have available?"
AI: "You have 10 tests configured:
    
    üåê Uptime Tests (tag: uptime):
    ‚Ä¢ netdata.helpmetest.com - Is netdata.helpmetest.com up?
    ‚Ä¢ login.helpmetest.com - Is login.helpmetest.com up?
    ‚Ä¢ ntfy.helpmetest.com - Is ntfy.helpmetest.com up?
    ‚Ä¢ helpmetest.com - Is helpmetest.com up?
    
    üîç Functional Tests:
    ‚Ä¢ Google search (tag: keywords)
    ‚Ä¢ /docs (tag: docs)
    
    üë§ User Journey Tests:
    ‚Ä¢ Login (tags: cuj, flaky)
    ‚Ä¢ Landing -> /docs (tags: landing, flaky)
    
    üìã Other Tests:
    ‚Ä¢ authorized
    ‚Ä¢ todos"

You: "Show me tests tagged with 'uptime'"
AI: "You have 4 uptime tests:
    ‚Ä¢ netdata.helpmetest.com (ID: w3yffj69d0j47bzcnj6eki)
    ‚Ä¢ login.helpmetest.com (ID: isopuekfypa3j3osz45kh9)
    ‚Ä¢ ntfy.helpmetest.com (ID: 0g5d9jbixfhg0p752t6z885d)
    ‚Ä¢ helpmetest.com (ID: pt178vwmfr5y9p3ev1dl9)
    
    These tests verify that your main services are responding."
```

### Test Execution by Tag
```
You: "Run all uptime tests"
AI: "Running all uptime tests...
    
    ‚úÖ All 4 uptime tests PASSED!
    
    Results:
    ‚Ä¢ netdata.helpmetest.com: ‚úÖ PASS (0.185s) - HTTP 200 OK
    ‚Ä¢ login.helpmetest.com: ‚úÖ PASS (0.127s) - HTTP 204 No Content
    ‚Ä¢ ntfy.helpmetest.com: ‚úÖ PASS (0.020s) - HTTP 200 OK
    ‚Ä¢ helpmetest.com: ‚úÖ PASS (0.022s) - HTTP 200 OK
    
    üìä Summary:
    ‚Ä¢ Total execution time: 0.35 seconds
    ‚Ä¢ Success rate: 100% (4/4 passed)
    ‚Ä¢ All your sites are UP and responding correctly!"

You: "Execute critical user journey tests"
AI: "Running tests tagged with 'cuj'...
    
    Results:
    ‚Ä¢ Login: ‚ùî UNKNOWN - Test hasn't run recently
    
    üìä Summary:
    ‚Ä¢ 1 test executed
    ‚Ä¢ Status: Unknown (may need configuration)
    ‚Ä¢ Recommendation: Check test setup for Login flow"
```

### Test Execution by Name
```
You: "Run the Google search test"
AI: "Running 'Google search' test...
    
    ‚ùå Test FAILED
    ‚Ä¢ Test: Google search (keywords tag)
    ‚Ä¢ Duration: 2.3 seconds
    ‚Ä¢ Issue: Search functionality test failed
    ‚Ä¢ Keywords executed: 3 passed, 1 failed
    
    The test completed but encountered issues with the search automation,
    likely due to rate limiting or CAPTCHA protection."

You: "Execute the netdata health check"
AI: "Running 'netdata.helpmetest.com' test...
    
    ‚úÖ Test PASSED!
    ‚Ä¢ Test: Is netdata.helpmetest.com up?
    ‚Ä¢ Duration: 0.185 seconds
    ‚Ä¢ Response: HTTP 200 OK in 17.9ms
    ‚Ä¢ Status: Healthy and responding normally"
```

### Test Execution by ID
```
You: "Run test ID w3yffj69d0j47bzcnj6eki"
AI: "Running test ID w3yffj69d0j47bzcnj6eki...
    
    ‚úÖ Test PASSED!
    ‚Ä¢ Test: netdata.helpmetest.com - Is netdata.helpmetest.com up?
    ‚Ä¢ Duration: 0.185 seconds
    ‚Ä¢ Response: HTTP 200 OK in 17.9ms
    ‚Ä¢ Keywords: GET http://netdata.helpmetest.com (PASS, 17.9ms)
    ‚Ä¢ Status: Service is healthy and responding normally"
```

## Use Cases

### Pre-Deployment Verification
```
You: "Before I deploy, run all critical tests"
AI: "I'll run your critical tests. Let me check what you have available...
    
    Running uptime tests (4 tests) and user journey tests (2 tests)...
    
    ‚úÖ Uptime Tests: 4/4 PASSED
    ‚úÖ User Journey Tests: 1/2 PASSED (1 unknown)
    
    üéØ Deployment Status: READY
    All critical services are responding. One user journey test 
    needs configuration but won't block deployment."
```

### Development Workflow
```
You: "Test the search functionality"
AI: "Running search-related tests...
    
    Found 1 test: 'Google search' (keywords tag)
    
    ‚ùå Test FAILED
    ‚Ä¢ Duration: 2.3 seconds
    ‚Ä¢ Issue: Search automation failed
    ‚Ä¢ Likely cause: Rate limiting or CAPTCHA
    
    üí° Suggestion: This test may need updating for current search behavior."
```

### Monitoring & Health Checks
```
You: "Run a quick health check on all services"
AI: "Running all uptime tests for service health check...
    
    ‚úÖ All services healthy!
    ‚Ä¢ netdata.helpmetest.com: 185ms
    ‚Ä¢ login.helpmetest.com: 127ms  
    ‚Ä¢ ntfy.helpmetest.com: 20ms
    ‚Ä¢ helpmetest.com: 22ms
    
    All response times are within normal ranges."
```

### Test Organization
```
You: "What test categories do I have?"
AI: "Your tests are organized into these categories:
    
    üåê uptime (4 tests) - Service availability
    üîç keywords (1 test) - Search functionality  
    üìÑ docs (1 test) - Documentation
    üöÄ landing (1 test) - Landing pages
    üë§ cuj (1 test) - Critical user journeys
    ‚ö†Ô∏è flaky (2 tests) - Tests that may be unstable
    
    You can run any category with: 'run [category] tests'"
```

## Advanced Features

### Detailed Execution Results
Each test execution provides:
- **Test Results**: Pass/fail status for each test
- **Performance Data**: Execution timing and response times
- **Keyword Logs**: Detailed step-by-step execution
- **Event Stream**: Complete test execution events
- **Error Analysis**: Detailed failure information when tests fail

### Tag-Based Organization
Tests can be organized using tags:
- `uptime` - Service availability tests
- `api` - API endpoint tests
- `cuj` - Critical user journey tests
- `keywords` - Search and functionality tests
- `docs` - Documentation tests
- `landing` - Landing page tests
- `flaky` - Tests that may be unstable

### Multiple Execution Methods
- **By Tag**: `tag:uptime` - Run all tests with specific tag
- **By Name**: `Google search` - Run specific test by name
- **By ID**: `w3yffj69d0j47bzcnj6eki` - Run test by unique ID

## Best Practices

### Test Organization
- Use descriptive tags to group related tests
- Keep test names clear and specific
- Document test purposes in descriptions

### AI Interaction
- Use natural language: "run uptime tests" vs "execute tag:uptime"
- Ask for test lists before running to understand what's available
- Use specific test names when you know them

### Development Workflow
- Run uptime tests before deployments
- Execute relevant test categories during development
- Use test results to guide debugging and fixes

## Troubleshooting

### No Tests Found
```
You: "What tests do I have?"
AI: "You currently have no tests configured. Tests are created through:
    ‚Ä¢ Robot Framework test files
    ‚Ä¢ HelpMeTest CLI test commands
    ‚Ä¢ Web interface test creation"
```

### Test Execution Failures
```
You: "Why did my test fail?"
AI: "The test failed because:
    ‚Ä¢ Network timeout (service not responding)
    ‚Ä¢ Authentication issues
    ‚Ä¢ Service returning error status
    ‚Ä¢ Test automation issues (CAPTCHA, rate limiting)
    
    Check the detailed logs for specific error messages."
```

### Performance Issues
```
You: "My tests are running slowly"
AI: "Test performance can be affected by:
    ‚Ä¢ Network latency to test targets
    ‚Ä¢ Service response times
    ‚Ä¢ Test complexity (browser automation vs simple HTTP)
    ‚Ä¢ Concurrent test execution
    
    Current execution time was X.X seconds, which is [normal/slow] for this test type."
```

## Integration Examples

### CI/CD Pipeline
```bash
# In your deployment script
echo "Running pre-deployment tests via AI..."
echo "run uptime tests" | your-ai-cli-tool

# Check exit code and proceed with deployment
if [ $? -eq 0 ]; then
    echo "Tests passed, proceeding with deployment"
    deploy.sh
else
    echo "Tests failed, aborting deployment"
    exit 1
fi
```

### Monitoring Alerts
```bash
# In your monitoring script
echo "check all critical services" | your-ai-cli-tool
```

### Development Workflow
```bash
# Before committing changes
echo "run tests related to my changes" | your-ai-cli-tool
```

---

The test management capabilities transform your AI assistant into a powerful test execution and monitoring companion, making it easy to verify system health, run test suites, and maintain quality through natural conversation.

## Natural Language Examples

### Test Discovery
```
You: "What tests do I have available?"
AI: "You have 10 tests configured:
    
    üåê Uptime Tests (tag: uptime):
    ‚Ä¢ netdata.helpmetest.com - Is netdata.helpmetest.com up?
    ‚Ä¢ login.helpmetest.com - Is login.helpmetest.com up?
    ‚Ä¢ ntfy.helpmetest.com - Is ntfy.helpmetest.com up?
    ‚Ä¢ helpmetest.com - Is helpmetest.com up?
    
    üîç Functional Tests:
    ‚Ä¢ Google search (tag: keywords)
    ‚Ä¢ /docs (tag: docs)
    
    üë§ User Journey Tests:
    ‚Ä¢ Login (tags: cuj, flaky)
    ‚Ä¢ Landing -> /docs (tags: landing, flaky)
    
    üìã Other Tests:
    ‚Ä¢ authorized
    ‚Ä¢ todos"

You: "Show me tests tagged with 'uptime'"
AI: "You have 4 uptime tests:
    ‚Ä¢ netdata.helpmetest.com (ID: w3yffj69d0j47bzcnj6eki)
    ‚Ä¢ login.helpmetest.com (ID: isopuekfypa3j3osz45kh9)
    ‚Ä¢ ntfy.helpmetest.com (ID: 0g5d9jbixfhg0p752t6z885d)
    ‚Ä¢ helpmetest.com (ID: pt178vwmfr5y9p3ev1dl9)
    
    These tests verify that your main services are responding."
```

### Test Execution by Tag
```
You: "Run all uptime tests"
AI: "Running all uptime tests...
    
    ‚úÖ All 4 uptime tests PASSED!
    
    Results:
    ‚Ä¢ netdata.helpmetest.com: ‚úÖ PASS (0.185s) - HTTP 200 OK
    ‚Ä¢ login.helpmetest.com: ‚úÖ PASS (0.127s) - HTTP 204 No Content
    ‚Ä¢ ntfy.helpmetest.com: ‚úÖ PASS (0.020s) - HTTP 200 OK
    ‚Ä¢ helpmetest.com: ‚úÖ PASS (0.022s) - HTTP 200 OK
    
    üìä Summary:
    ‚Ä¢ Total execution time: 0.35 seconds
    ‚Ä¢ Success rate: 100% (4/4 passed)
    ‚Ä¢ All your sites are UP and responding correctly!"

You: "Execute critical user journey tests"
AI: "Running tests tagged with 'cuj'...
    
    Results:
    ‚Ä¢ Login: ‚ùî UNKNOWN - Test hasn't run recently
    
    üìä Summary:
    ‚Ä¢ 1 test executed
    ‚Ä¢ Status: Unknown (may need configuration)
    ‚Ä¢ Recommendation: Check test setup for Login flow"
```

### Test Execution by Name
```
You: "Run the Google search test"
AI: "Running 'Google search' test...
    
    ‚ùå Test FAILED
    ‚Ä¢ Test: Google search (keywords tag)
    ‚Ä¢ Duration: 2.3 seconds
    ‚Ä¢ Issue: Search functionality test failed
    ‚Ä¢ Keywords executed: 3 passed, 1 failed
    
    The test completed but encountered issues with the search automation,
    likely due to rate limiting or CAPTCHA protection."

You: "Execute the netdata health check"
AI: "Running 'netdata.helpmetest.com' test...
    
    ‚úÖ Test PASSED!
    ‚Ä¢ Test: Is netdata.helpmetest.com up?
    ‚Ä¢ Duration: 0.185 seconds
    ‚Ä¢ Response: HTTP 200 OK in 17.9ms
    ‚Ä¢ Status: Healthy and responding normally"
```

### Test Execution by ID
```
You: "Run test ID w3yffj69d0j47bzcnj6eki"
AI: "Running test ID w3yffj69d0j47bzcnj6eki...
    
    ‚úÖ Test PASSED!
    ‚Ä¢ Test: netdata.helpmetest.com - Is netdata.helpmetest.com up?
    ‚Ä¢ Duration: 0.185 seconds
    ‚Ä¢ Response: HTTP 200 OK in 17.9ms
    ‚Ä¢ Keywords: GET http://netdata.helpmetest.com (PASS, 17.9ms)
    ‚Ä¢ Status: Service is healthy and responding normally"
```

## Use Cases

### Pre-Deployment Verification
```
You: "Before I deploy, run all critical tests"
AI: "I'll run your critical tests. Let me check what you have available...
    
    Running uptime tests (4 tests) and user journey tests (2 tests)...
    
    ‚úÖ Uptime Tests: 4/4 PASSED
    ‚úÖ User Journey Tests: 1/2 PASSED (1 unknown)
    
    üéØ Deployment Status: READY
    All critical services are responding. One user journey test 
    needs configuration but won't block deployment."
```

### Development Workflow
```
You: "Test the search functionality"
AI: "Running search-related tests...
    
    Found 1 test: 'Google search' (keywords tag)
    
    ‚ùå Test FAILED
    ‚Ä¢ Duration: 2.3 seconds
    ‚Ä¢ Issue: Search automation failed
    ‚Ä¢ Likely cause: Rate limiting or CAPTCHA
    
    üí° Suggestion: This test may need updating for current search behavior."
```

### Monitoring & Health Checks
```
You: "Run a quick health check on all services"
AI: "Running all uptime tests for service health check...
    
    ‚úÖ All services healthy!
    ‚Ä¢ netdata.helpmetest.com: 185ms
    ‚Ä¢ login.helpmetest.com: 127ms  
    ‚Ä¢ ntfy.helpmetest.com: 20ms
    ‚Ä¢ helpmetest.com: 22ms
    
    All response times are within normal ranges."
```

### Test Organization
```
You: "What test categories do I have?"
AI: "Your tests are organized into these categories:
    
    üåê uptime (4 tests) - Service availability
    üîç keywords (1 test) - Search functionality  
    üìÑ docs (1 test) - Documentation
    üöÄ landing (1 test) - Landing pages
    üë§ cuj (1 test) - Critical user journeys
    ‚ö†Ô∏è flaky (2 tests) - Tests that may be unstable
    
    You can run any category with: 'run [category] tests'"
```

## Advanced Features

### Detailed Execution Results
Each test execution provides:
- **Test Results**: Pass/fail status for each test
- **Performance Data**: Execution timing and response times
- **Keyword Logs**: Detailed step-by-step execution
- **Event Stream**: Complete test execution events
- **Error Analysis**: Detailed failure information when tests fail

### Tag-Based Organization
Tests can be organized using tags:
- `uptime` - Service availability tests
- `api` - API endpoint tests
- `cuj` - Critical user journey tests
- `keywords` - Search and functionality tests
- `docs` - Documentation tests
- `landing` - Landing page tests
- `flaky` - Tests that may be unstable

### Multiple Execution Methods
- **By Tag**: `tag:uptime` - Run all tests with specific tag
- **By Name**: `Google search` - Run specific test by name
- **By ID**: `w3yffj69d0j47bzcnj6eki` - Run test by unique ID

## Best Practices

### Test Organization
- Use descriptive tags to group related tests
- Keep test names clear and specific
- Document test purposes in descriptions

### AI Interaction
- Use natural language: "run uptime tests" vs "execute tag:uptime"
- Ask for test lists before running to understand what's available
- Use specific test names when you know them

### Development Workflow
- Run uptime tests before deployments
- Execute relevant test categories during development
- Use test results to guide debugging and fixes

## Troubleshooting

### No Tests Found
```
You: "What tests do I have?"
AI: "You currently have no tests configured. Tests are created through:
    ‚Ä¢ Robot Framework test files
    ‚Ä¢ HelpMeTest CLI test commands
    ‚Ä¢ Web interface test creation"
```

### Test Execution Failures
```
You: "Why did my test fail?"
AI: "The test failed because:
    ‚Ä¢ Network timeout (service not responding)
    ‚Ä¢ Authentication issues
    ‚Ä¢ Service returning error status
    ‚Ä¢ Test automation issues (CAPTCHA, rate limiting)
    
    Check the detailed logs for specific error messages."
```

### Performance Issues
```
You: "My tests are running slowly"
AI: "Test performance can be affected by:
    ‚Ä¢ Network latency to test targets
    ‚Ä¢ Service response times
    ‚Ä¢ Test complexity (browser automation vs simple HTTP)
    ‚Ä¢ Concurrent test execution
    
    Current execution time was X.X seconds, which is [normal/slow] for this test type."
```

## Integration Examples

### CI/CD Pipeline
```bash
# In your deployment script
echo "Running pre-deployment tests via AI..."
echo "run uptime tests" | your-ai-cli-tool

# Check exit code and proceed with deployment
if [ $? -eq 0 ]; then
    echo "Tests passed, proceeding with deployment"
    deploy.sh
else
    echo "Tests failed, aborting deployment"
    exit 1
fi
```

### Monitoring Alerts
```bash
# In your monitoring script
echo "check all critical services" | your-ai-cli-tool
```

### Development Workflow
```bash
# Before committing changes
echo "run tests related to my changes" | your-ai-cli-tool
```

---

The test management capabilities transform your AI assistant into a powerful test execution and monitoring companion, making it easy to verify system health, run test suites, and maintain quality through natural conversation.